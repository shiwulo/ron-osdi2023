--- qspinlock.c	2021-05-02 17:10:27
+++ qspinlock.c.new	2023-05-26 23:36:42
@@ -22,6 +22,7 @@
 #include <linux/prefetch.h>
 #include <asm/byteorder.h>
 #include <asm/qspinlock.h>
+#include <stdatomic.h>
 
 /*
  * Include queued spinlock statistics code
@@ -66,7 +67,7 @@
  */
 
 #include "mcs_spinlock.h"
-#define MAX_NODES	4
+#define MAX_NODES 4
 
 /*
  * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in
@@ -94,8 +95,117 @@
  * progress.
  */
 #ifndef _Q_PENDING_LOOPS
-#define _Q_PENDING_LOOPS	1
+#define _Q_PENDING_LOOPS 1
 #endif
+
+//for AMD 2990WX
+int idCov[64] = { 17, 48, 22, 6,  29, 50, 58, 55, 35, 4,  26, 10, 28,
+		  40, 12, 15, 57, 46, 54, 63, 25, 61, 37, 0,  14, 44,
+		  30, 49, 52, 1,  9,  38, 7,  34, 60, 33, 24, 20, 59,
+		  41, 5,  8,  45, 13, 43, 31, 36, 21, 19, 23, 51, 3,
+		  11, 32, 27, 39, 62, 18, 42, 47, 53, 16, 56, 2 };
+
+struct Plock {
+	atomic_t numWait;
+	atomic_t contextField[4];
+} __attribute__((aligned(8)));
+
+struct SpinlockAddress {
+	struct qspinlock *addr;
+};
+
+struct Plock wait_ary[128] __attribute__((aligned(64))) = { 0 };
+
+struct SpinlockAddress spinlockAddr[128][8]
+	__attribute__((aligned(64))) = { NULL };
+
+static __always_inline int getTspOrder(void)
+{
+	return idCov[smp_processor_id()];
+}
+
+long over_count = 0;
+long under_count = 0;
+
+void _spin_０lock(struct qspinlock *spin)
+{
+	int tspOrder = getTspOrder();
+	//int zero = 0;
+	atomic_int_fast32_t zero32;
+	atomic_int zero;
+
+	int context = atomic_fetch_add_explicit(
+		(int *)&wait_ary[tspOrder].numWait, 1, memory_order_relaxed);
+
+	if (unlikely(context >= 4)) {
+		while (1) {
+			while (spin->val.counter != 0)
+				cpu_relax();
+
+			zero32 = 0;
+			if (atomic_compare_exchange_weak_explicit(
+				    (atomic_int_fast32_t *)spin, &zero32, 1,
+				    memory_order_relaxed,
+				    memory_order_relaxed)) {
+				//spinlockAddr[tspOrder][context].addr = NULL;
+				//wait_ary[tspOrder].numWait.counter = 1;
+				return;
+				//goto release;
+			}
+		}
+	}
+	spinlockAddr[tspOrder][context].addr = spin;
+
+	while (1) {
+		while (wait_ary[tspOrder].contextField[context].counter == 0 &&
+		       spin->val.counter == 1)
+			asm("pause");
+
+		zero = 1;
+		if (atomic_compare_exchange_weak_explicit(
+			    (atomic_int *)&wait_ary[tspOrder]
+				    .contextField[context],
+			    &zero, 0, memory_order_relaxed,
+			    memory_order_relaxed)) {
+			return;
+		}
+
+		zero32 = 0;
+		if (atomic_compare_exchange_weak_explicit(
+			    (atomic_int_fast32_t *)spin, &zero32, 1,
+			    memory_order_relaxed, memory_order_relaxed)) {
+			spinlockAddr[tspOrder][context].addr = NULL;
+			return;
+		}
+	}
+}
+
+long numUnlock = 0;
+
+void _spin_unlock(struct qspinlock *spin)
+{
+	int i;
+	int tspOrder = getTspOrder();
+	numUnlock++;
+	atomic_fetch_sub_explicit((int *)&wait_ary[tspOrder].numWait, 1,
+				  memory_order_release);
+
+	for (i = 0; i < 64; i++) {
+		int idx = (tspOrder + i) % 64;
+		if (wait_ary[idx].numWait.counter > 0) {
+			int j;
+			for (j = 3; j >= 0; j--) {
+				if (spinlockAddr[idx][j].addr == spin) {
+					wait_ary[idx].contextField[j].counter =
+						1;
+					return;
+				}
+			}
+		}
+	}
+	atomic_store_explicit((atomic_int_fast32_t *)spin, 0,
+			      memory_order_relaxed);
+}
 
 /*
  * Per-CPU queue node structures; we can never have more than 4 nested
@@ -116,7 +226,7 @@
 {
 	u32 tail;
 
-	tail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;
+	tail = (cpu + 1) << _Q_TAIL_CPU_OFFSET;
 	tail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */
 
 	return tail;
@@ -125,13 +235,13 @@
 static inline __pure struct mcs_spinlock *decode_tail(u32 tail)
 {
 	int cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;
-	int idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;
+	int idx = (tail & _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;
 
 	return per_cpu_ptr(&qnodes[idx].mcs, cpu);
 }
 
-static inline __pure
-struct mcs_spinlock *grab_mcs_node(struct mcs_spinlock *base, int idx)
+static inline __pure struct mcs_spinlock *
+grab_mcs_node(struct mcs_spinlock *base, int idx)
 {
 	return &((struct qnode *)base + idx)->mcs;
 }
@@ -176,11 +286,11 @@
 static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)
 {
 	/*
-	 * We can use relaxed semantics since the caller ensures that the
-	 * MCS node is properly initialized before updating the tail.
-	 */
-	return (u32)xchg_relaxed(&lock->tail,
-				 tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;
+     * We can use relaxed semantics since the caller ensures that the
+     * MCS node is properly initialized before updating the tail.
+     */
+	return (u32)xchg_relaxed(&lock->tail, tail >> _Q_TAIL_OFFSET)
+	       << _Q_TAIL_OFFSET;
 }
 
 #else /* _Q_PENDING_BITS == 8 */
@@ -224,10 +334,10 @@
 	for (;;) {
 		new = (val & _Q_LOCKED_PENDING_MASK) | tail;
 		/*
-		 * We can use relaxed semantics since the caller ensures that
-		 * the MCS node is properly initialized before updating the
-		 * tail.
-		 */
+	 * We can use relaxed semantics since the caller ensures that
+	 * the MCS node is properly initialized before updating the
+	 * tail.
+	 */
 		old = atomic_cmpxchg_relaxed(&lock->val, val, new);
 		if (old == val)
 			break;
@@ -246,7 +356,8 @@
  * *,*,* -> *,1,*
  */
 #ifndef queued_fetch_set_pending_acquire
-static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)
+static __always_inline u32
+queued_fetch_set_pending_acquire(struct qspinlock *lock)
 {
 	return atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);
 }
@@ -263,30 +374,40 @@
 	WRITE_ONCE(lock->locked, _Q_LOCKED_VAL);
 }
 
-
 /*
  * Generate the native code for queued_spin_unlock_slowpath(); provide NOPs for
  * all the PV callbacks.
  */
 
-static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }
+static __always_inline void __pv_init_node(struct mcs_spinlock *node)
+{
+}
+
 static __always_inline void __pv_wait_node(struct mcs_spinlock *node,
-					   struct mcs_spinlock *prev) { }
+					   struct mcs_spinlock *prev)
+{
+}
+
 static __always_inline void __pv_kick_node(struct qspinlock *lock,
-					   struct mcs_spinlock *node) { }
-static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,
-						   struct mcs_spinlock *node)
-						   { return 0; }
+					   struct mcs_spinlock *node)
+{
+}
 
-#define pv_enabled()		false
+static __always_inline u32 __pv_wait_head_or_lock(struct qspinlock *lock,
+						  struct mcs_spinlock *node)
+{
+	return 0;
+}
 
-#define pv_init_node		__pv_init_node
-#define pv_wait_node		__pv_wait_node
-#define pv_kick_node		__pv_kick_node
-#define pv_wait_head_or_lock	__pv_wait_head_or_lock
+#define pv_enabled() false
 
+#define pv_init_node __pv_init_node
+#define pv_wait_node __pv_wait_node
+#define pv_kick_node __pv_kick_node
+#define pv_wait_head_or_lock __pv_wait_head_or_lock
+
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
-#define queued_spin_lock_slowpath	native_queued_spin_lock_slowpath
+#define queued_spin_lock_slowpath native_queued_spin_lock_slowpath
 #endif
 
 #endif /* _GEN_PV_LOCK_SLOWPATH */
@@ -327,39 +448,38 @@
 		return;
 
 	/*
-	 * Wait for in-progress pending->locked hand-overs with a bounded
-	 * number of spins so that we guarantee forward progress.
-	 *
-	 * 0,1,0 -> 0,0,1
-	 */
+     * Wait for in-progress pending->locked hand-overs with a bounded
+     * number of spins so that we guarantee forward progress.
+     *
+     * 0,1,0 -> 0,0,1
+     */
 	if (val == _Q_PENDING_VAL) {
 		int cnt = _Q_PENDING_LOOPS;
-		val = atomic_cond_read_relaxed(&lock->val,
-					       (VAL != _Q_PENDING_VAL) || !cnt--);
+		val = atomic_cond_read_relaxed(
+			&lock->val, (VAL != _Q_PENDING_VAL) || !cnt--);
 	}
 
 	/*
-	 * If we observe any contention; queue.
-	 */
+     * If we observe any contention; queue.
+     */
 	if (val & ~_Q_LOCKED_MASK)
 		goto queue;
 
 	/*
-	 * trylock || pending
-	 *
-	 * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock
-	 */
+     * trylock || pending
+     *
+     * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock
+     */
 	val = queued_fetch_set_pending_acquire(lock);
 
 	/*
-	 * If we observe contention, there is a concurrent locker.
-	 *
-	 * Undo and queue; our setting of PENDING might have made the
-	 * n,0,0 -> 0,0,0 transition fail and it will now be waiting
-	 * on @next to become !NULL.
-	 */
+     * If we observe contention, there is a concurrent locker.
+     *
+     * Undo and queue; our setting of PENDING might have made the
+     * n,0,0 -> 0,0,0 transition fail and it will now be waiting
+     * on @next to become !NULL.
+     */
 	if (unlikely(val & ~_Q_LOCKED_MASK)) {
-
 		/* Undo PENDING if we set it. */
 		if (!(val & _Q_PENDING_MASK))
 			clear_pending(lock);
@@ -368,32 +488,32 @@
 	}
 
 	/*
-	 * We're pending, wait for the owner to go away.
-	 *
-	 * 0,1,1 -> 0,1,0
-	 *
-	 * this wait loop must be a load-acquire such that we match the
-	 * store-release that clears the locked bit and create lock
-	 * sequentiality; this is because not all
-	 * clear_pending_set_locked() implementations imply full
-	 * barriers.
-	 */
+     * We're pending, wait for the owner to go away.
+     *
+     * 0,1,1 -> 0,1,0
+     *
+     * this wait loop must be a load-acquire such that we match the
+     * store-release that clears the locked bit and create lock
+     * sequentiality; this is because not all
+     * clear_pending_set_locked() implementations imply full
+     * barriers.
+     */
 	if (val & _Q_LOCKED_MASK)
 		atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_MASK));
 
 	/*
-	 * take ownership and clear the pending bit.
-	 *
-	 * 0,1,0 -> 0,0,1
-	 */
+     * take ownership and clear the pending bit.
+     *
+     * 0,1,0 -> 0,0,1
+     */
 	clear_pending_set_locked(lock);
 	lockevent_inc(lock_pending);
 	return;
 
 	/*
-	 * End of pending bit optimistic spinning and beginning of MCS
-	 * queuing.
-	 */
+     * End of pending bit optimistic spinning and beginning of MCS
+     * queuing.
+     */
 queue:
 	lockevent_inc(lock_slowpath);
 pv_queue:
@@ -402,14 +522,14 @@
 	tail = encode_tail(smp_processor_id(), idx);
 
 	/*
-	 * 4 nodes are allocated based on the assumption that there will
-	 * not be nested NMIs taking spinlocks. That may not be true in
-	 * some architectures even though the chance of needing more than
-	 * 4 nodes will still be extremely unlikely. When that happens,
-	 * we fall back to spinning on the lock directly without using
-	 * any MCS node. This is not the most elegant solution, but is
-	 * simple enough.
-	 */
+     * 4 nodes are allocated based on the assumption that there will
+     * not be nested NMIs taking spinlocks. That may not be true in
+     * some architectures even though the chance of needing more than
+     * 4 nodes will still be extremely unlikely. When that happens,
+     * we fall back to spinning on the lock directly without using
+     * any MCS node. This is not the most elegant solution, but is
+     * simple enough.
+     */
 	if (unlikely(idx >= MAX_NODES)) {
 		lockevent_inc(lock_no_node);
 		while (!queued_spin_trylock(lock))
@@ -420,15 +540,15 @@
 	node = grab_mcs_node(node, idx);
 
 	/*
-	 * Keep counts of non-zero index values:
-	 */
+     * Keep counts of non-zero index values:
+     */
 	lockevent_cond_inc(lock_use_node2 + idx - 1, idx);
 
 	/*
-	 * Ensure that we increment the head node->count before initialising
-	 * the actual node. If the compiler is kind enough to reorder these
-	 * stores, then an IRQ could overwrite our assignments.
-	 */
+     * Ensure that we increment the head node->count before initialising
+     * the actual node. If the compiler is kind enough to reorder these
+     * stores, then an IRQ could overwrite our assignments.
+     */
 	barrier();
 
 	node->locked = 0;
@@ -436,34 +556,34 @@
 	pv_init_node(node);
 
 	/*
-	 * We touched a (possibly) cold cacheline in the per-cpu queue node;
-	 * attempt the trylock once more in the hope someone let go while we
-	 * weren't watching.
-	 */
+     * We touched a (possibly) cold cacheline in the per-cpu queue node;
+     * attempt the trylock once more in the hope someone let go while we
+     * weren't watching.
+     */
 	if (queued_spin_trylock(lock))
 		goto release;
 
 	/*
-	 * Ensure that the initialisation of @node is complete before we
-	 * publish the updated tail via xchg_tail() and potentially link
-	 * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.
-	 */
+     * Ensure that the initialisation of @node is complete before we
+     * publish the updated tail via xchg_tail() and potentially link
+     * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.
+     */
 	smp_wmb();
 
 	/*
-	 * Publish the updated tail.
-	 * We have already touched the queueing cacheline; don't bother with
-	 * pending stuff.
-	 *
-	 * p,*,* -> n,*,*
-	 */
+     * Publish the updated tail.
+     * We have already touched the queueing cacheline; don't bother with
+     * pending stuff.
+     *
+     * p,*,* -> n,*,*
+     */
 	old = xchg_tail(lock, tail);
 	next = NULL;
 
 	/*
-	 * if there was a previous node; link it and wait until reaching the
-	 * head of the waitqueue.
-	 */
+     * if there was a previous node; link it and wait until reaching the
+     * head of the waitqueue.
+     */
 	if (old & _Q_TAIL_MASK) {
 		prev = decode_tail(old);
 
@@ -474,79 +594,80 @@
 		arch_mcs_spin_lock_contended(&node->locked);
 
 		/*
-		 * While waiting for the MCS lock, the next pointer may have
-		 * been set by another lock waiter. We optimistically load
-		 * the next pointer & prefetch the cacheline for writing
-		 * to reduce latency in the upcoming MCS unlock operation.
-		 */
+	 * While waiting for the MCS lock, the next pointer may have
+	 * been set by another lock waiter. We optimistically load
+	 * the next pointer & prefetch the cacheline for writing
+	 * to reduce latency in the upcoming MCS unlock operation.
+	 */
 		next = READ_ONCE(node->next);
 		if (next)
 			prefetchw(next);
 	}
 
 	/*
-	 * we're at the head of the waitqueue, wait for the owner & pending to
-	 * go away.
-	 *
-	 * *,x,y -> *,0,0
-	 *
-	 * this wait loop must use a load-acquire such that we match the
-	 * store-release that clears the locked bit and create lock
-	 * sequentiality; this is because the set_locked() function below
-	 * does not imply a full barrier.
-	 *
-	 * The PV pv_wait_head_or_lock function, if active, will acquire
-	 * the lock and return a non-zero value. So we have to skip the
-	 * atomic_cond_read_acquire() call. As the next PV queue head hasn't
-	 * been designated yet, there is no way for the locked value to become
-	 * _Q_SLOW_VAL. So both the set_locked() and the
-	 * atomic_cmpxchg_relaxed() calls will be safe.
-	 *
-	 * If PV isn't active, 0 will be returned instead.
-	 *
-	 */
+     * we're at the head of the waitqueue, wait for the owner & pending to
+     * go away.
+     *
+     * *,x,y -> *,0,0
+     *
+     * this wait loop must use a load-acquire such that we match the
+     * store-release that clears the locked bit and create lock
+     * sequentiality; this is because the set_locked() function below
+     * does not imply a full barrier.
+     *
+     * The PV pv_wait_head_or_lock function, if active, will acquire
+     * the lock and return a non-zero value. So we have to skip the
+     * atomic_cond_read_acquire() call. As the next PV queue head hasn't
+     * been designated yet, there is no way for the locked value to become
+     * _Q_SLOW_VAL. So both the set_locked() and the
+     * atomic_cmpxchg_relaxed() calls will be safe.
+     *
+     * If PV isn't active, 0 will be returned instead.
+     *
+     */
 	if ((val = pv_wait_head_or_lock(lock, node)))
 		goto locked;
 
-	val = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));
+	val = atomic_cond_read_acquire(&lock->val,
+				       !(VAL & _Q_LOCKED_PENDING_MASK));
 
 locked:
 	/*
-	 * claim the lock:
-	 *
-	 * n,0,0 -> 0,0,1 : lock, uncontended
-	 * *,*,0 -> *,*,1 : lock, contended
-	 *
-	 * If the queue head is the only one in the queue (lock value == tail)
-	 * and nobody is pending, clear the tail code and grab the lock.
-	 * Otherwise, we only need to grab the lock.
-	 */
+     * claim the lock:
+     *
+     * n,0,0 -> 0,0,1 : lock, uncontended
+     * *,*,0 -> *,*,1 : lock, contended
+     *
+     * If the queue head is the only one in the queue (lock value == tail)
+     * and nobody is pending, clear the tail code and grab the lock.
+     * Otherwise, we only need to grab the lock.
+     */
 
 	/*
-	 * In the PV case we might already have _Q_LOCKED_VAL set, because
-	 * of lock stealing; therefore we must also allow:
-	 *
-	 * n,0,1 -> 0,0,1
-	 *
-	 * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the
-	 *       above wait condition, therefore any concurrent setting of
-	 *       PENDING will make the uncontended transition fail.
-	 */
+     * In the PV case we might already have _Q_LOCKED_VAL set, because
+     * of lock stealing; therefore we must also allow:
+     *
+     * n,0,1 -> 0,0,1
+     *
+     * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the
+     *       above wait condition, therefore any concurrent setting of
+     *       PENDING will make the uncontended transition fail.
+     */
 	if ((val & _Q_TAIL_MASK) == tail) {
 		if (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))
 			goto release; /* No contention */
 	}
 
 	/*
-	 * Either somebody is queued behind us or _Q_PENDING_VAL got set
-	 * which will then detect the remaining tail and queue behind us
-	 * ensuring we'll see a @next.
-	 */
+     * Either somebody is queued behind us or _Q_PENDING_VAL got set
+     * which will then detect the remaining tail and queue behind us
+     * ensuring we'll see a @next.
+     */
 	set_locked(lock);
 
 	/*
-	 * contended path; wait for next if not observed yet, release.
-	 */
+     * contended path; wait for next if not observed yet, release.
+     */
 	if (!next)
 		next = smp_cond_load_relaxed(&node->next, (VAL));
 
@@ -555,10 +676,11 @@
 
 release:
 	/*
-	 * release the node
-	 */
+     * release the node
+     */
 	__this_cpu_dec(qnodes[0].mcs.count);
 }
+
 EXPORT_SYMBOL(queued_spin_lock_slowpath);
 
 /*
@@ -567,16 +689,16 @@
 #if !defined(_GEN_PV_LOCK_SLOWPATH) && defined(CONFIG_PARAVIRT_SPINLOCKS)
 #define _GEN_PV_LOCK_SLOWPATH
 
-#undef  pv_enabled
-#define pv_enabled()	true
+#undef pv_enabled
+#define pv_enabled() true
 
 #undef pv_init_node
 #undef pv_wait_node
 #undef pv_kick_node
 #undef pv_wait_head_or_lock
 
-#undef  queued_spin_lock_slowpath
-#define queued_spin_lock_slowpath	__pv_queued_spin_lock_slowpath
+#undef queued_spin_lock_slowpath
+#define queued_spin_lock_slowpath __pv_queued_spin_lock_slowpath
 
 #include "qspinlock_paravirt.h"
 #include "qspinlock.c"
@@ -587,5 +709,6 @@
 	nopvspin = true;
 	return 0;
 }
+
 early_param("nopvspin", parse_nopvspin);
 #endif
